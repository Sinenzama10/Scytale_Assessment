{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24027b4e-3b39-416d-8112-05177f6292cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"updated_at\", TimestampType(), True),\n",
    "    StructField(\"closed_at\", TimestampType(), True),\n",
    "    StructField(\"merged_at\", TimestampType(), True),\n",
    "    StructField(\"user\", StructType([\n",
    "        StructField(\"login\", StringType(), True),\n",
    "    ]), True),\n",
    "    StructField(\"head\", StructType([\n",
    "        StructField(\"repo\", StructType([\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"full_name\", StringType(), True),\n",
    "        ]), True),\n",
    "    ]), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216b41b3-a09a-41e7-91ee-30fbc4d97f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/04 13:13:39 WARN Utils: Your hostname, Sinethembas-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.16.2.0 instead (on interface en0)\n",
      "24/02/04 13:13:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/04 13:13:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, max, split\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Process GitHub Repos\").getOrCreate()\n",
    "\n",
    "# Specify the folder containing JSON files\n",
    "folder_path = \"../data/*.json\"  # Use wildcard to select all JSON files\n",
    "\n",
    "# Read all JSON files in the folder using the defined schema\n",
    "df = spark.read.schema(schema).json(folder_path)\n",
    "\n",
    "# Filter out empty DataFrames by checking if there are any rows\n",
    "if df.rdd.isEmpty():\n",
    "    print(\"No data found in any file.\")\n",
    "else:\n",
    "    # Proceed with processing if the DataFrame is not empty\n",
    "    transformed_df = df.withColumn(\"Organization Name\", split(col(\"head.repo.full_name\"), \"/\")[0]) \\\n",
    "        .withColumn(\"repository_id\", col(\"head.repo.id\")) \\\n",
    "        .withColumn(\"repository_name\", col(\"head.repo.name\")) \\\n",
    "        .withColumn(\"repository_owner\", col(\"user.login\")) \\\n",
    "        .groupBy(\"Organization Name\", \"repository_id\", \"repository_name\", \"repository_owner\") \\\n",
    "        .agg(\n",
    "            count(\"id\").alias(\"num_prs\"),\n",
    "            count(when(col(\"state\") == \"MERGED\", True)).alias(\"num_prs_merged\"),\n",
    "            max(\"merged_at\").alias(\"merged_at\")\n",
    "        ) \\\n",
    "        .withColumn(\"is_compliant\", \n",
    "                    ((col(\"num_prs\") == col(\"num_prs_merged\")) & col(\"repository_owner\").contains(\"scytale\")).cast(\"boolean\"))\n",
    "\n",
    "    # Save the transformed DataFrame to a Parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a4a6196-05b2-4c1e-b478-680f0b711b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Organization Name='Scytale-exercise', repository_id='721612130', repository_name='scytale-repo3', repository_owner='aviyashar', num_prs=4, num_prs_merged=0, merged_at=datetime.datetime(2023, 11, 21, 14, 29, 7), is_compliant=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10dd5a0c-09fa-4024-a0e5-6b0543588e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+----------------+-------+--------------+-------------------+------------+\n",
      "|Organization Name|repository_id|repository_name|repository_owner|num_prs|num_prs_merged|merged_at          |is_compliant|\n",
      "+-----------------+-------------+---------------+----------------+-------+--------------+-------------------+------------+\n",
      "|Scytale-exercise |721612130    |scytale-repo3  |aviyashar       |4      |0             |2023-11-21 14:29:07|false       |\n",
      "|Scytale-exercise |724133322    |Scytale_repo   |aviyashar       |2      |0             |NULL               |false       |\n",
      "|Scytale-exercise |724140378    |scytale-repo2  |aviyashar       |1      |0             |2023-11-27 15:34:05|false       |\n",
      "+-----------------+-------------+---------------+----------------+-------+--------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After your transformations are complete\n",
    "transformed_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec69f75-8bcf-4e1a-a8d9-81b14ee33310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed_df.write.mode(\"overwrite\").parquet(\"../output/prs_aggregated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2bbf4e-f68e-4a7d-abaa-759278ba53a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- repository_id: string (nullable = true)\n",
      " |-- repository_name: string (nullable = true)\n",
      " |-- repository_owner: string (nullable = true)\n",
      " |-- num_prs: long (nullable = true)\n",
      " |-- num_prs_merged: long (nullable = true)\n",
      " |-- latest_merged_at: timestamp (nullable = true)\n",
      " |-- is_compliant: boolean (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      "\n",
      "+-------------+---------------+----------------+-------+--------------+-------------------+------------+-----------------+\n",
      "|repository_id|repository_name|repository_owner|num_prs|num_prs_merged|   latest_merged_at|is_compliant|Organization Name|\n",
      "+-------------+---------------+----------------+-------+--------------+-------------------+------------+-----------------+\n",
      "|    721612130|  scytale-repo3|       aviyashar|      4|             0|2023-11-21 14:29:07|       false| Scytale-exercise|\n",
      "|    724133322|   Scytale_repo|       aviyashar|      2|             0|               NULL|       false| Scytale-exercise|\n",
      "|    724140378|  scytale-repo2|       aviyashar|      1|             0|2023-11-27 15:34:05|       false| Scytale-exercise|\n",
      "+-------------+---------------+----------------+-------+--------------+-------------------+------------+-----------------+\n",
      "\n",
      "Row count: 3\n"
     ]
    }
   ],
   "source": [
    "#Check if the parquet file has been correctly output and can be read back into a dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ParquetFileCheck\").getOrCreate()\n",
    "\n",
    "# Path to your parquet file\n",
    "parquet_file_path = '../output/prs_aggregated.parquet'\n",
    "\n",
    "# Read the parquet file into a DataFrame\n",
    "parquet_df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Show the DataFrame schema to verify structure\n",
    "parquet_df.printSchema()\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "parquet_df.show()\n",
    "\n",
    "# If you just want to verify the row count\n",
    "print(\"Row count:\", parquet_df.count())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
